---
description: Performance optimization patterns for data processing
globs: ["**/processors/**/*", "**/extractors/**/*", "**/loaders/**/*"]
alwaysApply: false
---

# Python Performance Standards

## Library Hierarchy

**Always prefer: DuckDB > Polars > Pandas**

1. **DuckDB**: Analytical queries, aggregations, joins, window functions, direct file reading
2. **Polars**: ETL operations, transformations, lazy evaluation
3. **Pandas**: Only for external library compatibility (document why)

## DuckDB (Preferred)

```python
# Query files directly without loading into memory
conn.execute("""
    SELECT category, SUM(amount) FROM read_parquet('data/*.parquet')
    WHERE date >= '2024-01-01'
    GROUP BY category
""").fetchdf()
```

## Polars (When DuckDB SQL Isn't Suitable)

```python
df = (
    pl.read_parquet(file_path)
    .lazy()  # Lazy evaluation for better performance
    .filter(pl.col("amount") < 0)
    .group_by("category").agg(pl.col("amount").sum())
    .collect()
)
```

## Pandas (Last Resort)

```python
# Only when required by external libraries - document why
# Note: Using Pandas because external_lib requires pd.DataFrame
df = pd.read_parquet(file_path)
df['amount_abs'] = df['amount'].abs()  # Use vectorized ops, never loops
```

## Best Practices

- **Vectorized operations**: Never use loops for data processing
- **Memory management**: Use context managers, release large objects explicitly
- **Batch operations**: Insert/update in batches for better performance
- **Profile code**: Use `cProfile` or timing context managers to identify bottlenecks
- **Document library choice**: When using Pandas, explain why DuckDB/Polars weren't suitable
